{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "### YAML Configugarion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "## YAML Configugarion\n",
    "################################################\n",
    "\"\"\"\n",
    "name: volume_forecast_oreilly\n",
    "dependencies:\n",
    "  - python\n",
    "  - matplotlib\n",
    "  - mxnet \n",
    "  - numpy\n",
    "  - pandas\n",
    "  - scipy\n",
    "  - ipykernel\n",
    "  - openpyxl \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "width = 6\n",
    "height = 3\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [width, height]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## utilities\n",
    "import os\n",
    "\n",
    "## deep learning module\n",
    "import mxnet as mx\n",
    "\n",
    "## data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## reporting\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some hyperparameters we won't tune via command line inputs\n",
    "DATA_SEGMENTS    = { 'tr': 0.6, 'va': 0.2, 'tst': 0.2}\n",
    "THRESHOLD_EPOCHS = 2\n",
    "COR_THRESHOLD    =  0.005\n",
    "\n",
    "## temporal slicing\n",
    "WIN              = 24 * 7\n",
    "H                = 8\n",
    "\n",
    "## model details \n",
    "MODEL            = 'simple_lstnet_model'\n",
    "# MODEL            = 'fc_model' # not working\n",
    "# MODEL            = 'rnn_model'\n",
    "# MODEL            = 'cnn_model' # not working\n",
    "SZ_FILT          = 8\n",
    "N_FILT           = 10\n",
    "RNN_UNITS        = 10\n",
    "SEASONAL_PERIOD  = 24\n",
    "\n",
    "## training details\n",
    "GPU              = 0\n",
    "BATCH_N          = 64\n",
    "LR               = 0.0001\n",
    "DROP             = 0.2\n",
    "N_EPOCHS         = 300\n",
    "\n",
    "## data details\n",
    "# SHEET_NAME       = 'CC'\n",
    "SHEET_NAME       = 'HTU'\n",
    "DATA_FILE_DIR    = r'C:/Users/mjonkus/Philip Morris International/Finance Advanced Analytics - Finance - Global - General/03. Projects/4. IMS Forecasting/'\n",
    "DATA_FILE_NAME   = 'Daily IMS.xlsx'\n",
    "DATA_FILE        = DATA_FILE_DIR + DATA_FILE_NAME\n",
    "SAVE_DIR         = \"./IMS_NN_resultsDir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htu_IMS_raw = pd.read_excel(DATA_FILE,sheet_name=SHEET_NAME,engine='openpyxl')\n",
    "htu_IMS = htu_IMS_raw\n",
    "# htu_IMS = htu_IMS_raw.iloc[:,1:9]\n",
    "# htu_IMS.columns = ['time_stamp','HTU_IMS']\n",
    "# htu_IMS['Row.Labels'] = pd.to_datetime(htu_IMS['Row.Labels'])\n",
    "htu_IMS = htu_IMS.set_index('Row.Labels',drop=True) \n",
    "\n",
    "htu_IMS.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= htu_IMS.resample('D').asfreq().fillna(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator to resample time series and fill in with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generator to resample time series and fill in with 0\n",
    "\n",
    "def generate_missing_dates (df):\n",
    "    df_generated= df.resample('D').asfreq().fillna(0)\n",
    "    return df_generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ring bufffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RingBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.data = [0 for i in range(size)]\n",
    "\n",
    "    def append(self, x):\n",
    "        self.data.pop(0)\n",
    "        self.data.append(x)\n",
    "\n",
    "    def get(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "## DATA PREPARATION ##\n",
    "################################\n",
    "\n",
    "\n",
    "\n",
    "def prepared_data(data_file, win, h, model_name):\n",
    "    df = pd.read_excel(data_file,sheet_name=SHEET_NAME,engine='openpyxl')\n",
    "    df = df.set_index('Row.Labels',drop=True) \n",
    "    df = generate_missing_dates(df)\n",
    "    x  = df.iloc[:,1:8].values ## need to drop first column as that's an index not a value and keep only total PMI\n",
    "    x = (x - np.mean(x, axis = 0)) / (np.std(x, axis = 0)) ## normalize data\n",
    "    \n",
    "    if model_name == 'fc_model':\n",
    "        ## provide first and second step lookbacks in one flat input\n",
    "        X = np.hstack([x[1:-h], x[0:-(h+1)]])\n",
    "        Y = x[(h+1):]\n",
    "        return (X, Y)\n",
    "    else:    \n",
    "        # preallocate X and Y data arrays\n",
    "        # X shape = num examples * time win * num channels (NTC)\n",
    "        X = np.zeros((x.shape[0] - win - h, win, x.shape[1]))\n",
    "        # Y shape = num examples * num channels\n",
    "        Y = np.zeros((x.shape[0] - win - h, x.shape[1]))\n",
    "        \n",
    "        for i in range(win, x.shape[0] - h):\n",
    "            y_i = x[i + h - 1     , :] ## the target value is h steps ahead\n",
    "            x_i = x[(i - win) : i , :] ## the input data are the previous win steps\n",
    "            X[i-win] = x_i\n",
    "            Y[i-win] = y_i\n",
    "\n",
    "        return (X, Y)\n",
    "\n",
    "\n",
    "def prepare_iters(data_file, win, h, model, batch_n):\n",
    "    X, Y = prepared_data(data_file, win, h, model)\n",
    "\n",
    "    n_tr = int(Y.shape[0] * DATA_SEGMENTS['tr'])\n",
    "    n_va = int(Y.shape[0] * DATA_SEGMENTS['va'])\n",
    "\n",
    "    X_tr, X_valid, X_test = X[                 : n_tr], \\\n",
    "                            X[n_tr             : n_tr + n_va], \\\n",
    "                            X[n_tr + n_va : ]\n",
    "    Y_tr, Y_valid, Y_test = Y[                 : n_tr], \\\n",
    "                            Y[n_tr             : n_tr + n_va], \\\n",
    "                            Y[n_tr + n_va : ]\n",
    "    \n",
    "    iter_tr = mx.io.NDArrayIter(   data       = X_tr,\n",
    "                                   label      = Y_tr,\n",
    "                                   batch_size = batch_n)\n",
    "    iter_val = mx.io.NDArrayIter(  data       = X_valid,\n",
    "                                   label      = Y_valid,\n",
    "                                   batch_size = batch_n)\n",
    "    iter_test = mx.io.NDArrayIter( data       = X_test,\n",
    "                                   label      = Y_test,\n",
    "                                   batch_size = batch_n)\n",
    "\n",
    "    return (iter_tr, iter_val, iter_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model part\n",
    "### Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## MODELS ##\n",
    "################\n",
    "\n",
    "def fc_model(iter_train, input_feature_shape, X, Y,\n",
    "             win, sz_filt, n_filter, drop, seasonal_period):\n",
    "    output = mx.sym.FullyConnected(data=X, num_hidden=20)\n",
    "    output = mx.sym.Activation(output, act_type = 'relu')\n",
    "    output = mx.sym.FullyConnected(data=output, num_hidden=10)\n",
    "    output = mx.sym.Activation(output, act_type = 'relu')\n",
    "    output = mx.sym.FullyConnected(data=output, num_hidden=7)\n",
    "    \n",
    "    loss_grad = mx.sym.LinearRegressionOutput(data=output, label=Y)\n",
    "    return (loss_grad,\n",
    "            [v.name for v in iter_train.provide_data],\n",
    "            [v.name for v in iter_train.provide_label])    \n",
    "    \n",
    "def cnn_model(iter_train, input_feature_shape, X, Y,\n",
    "              win, sz_filt, n_filter, drop, seasonal_period):\n",
    "    conv_input = mx.sym.reshape(data=X, shape=(0, 1, win, -1)) \n",
    "    ## Convolution expects 4d input (N x channel x height x width)\n",
    "    ## in our case channel = 1 (similar to a black and white image\n",
    "    ## height = time and width = channels slash electric locations\n",
    "    \n",
    "    cnn_output = mx.sym.Convolution(data=conv_input,\n",
    "                                    kernel=(sz_filt,\n",
    "                                            input_feature_shape[2]),\n",
    "                                    num_filter=n_filter)\n",
    "    cnn_output = mx.sym.Activation(data=cnn_output, act_type='relu')\n",
    "    cnn_output = mx.sym.reshape(mx.sym.transpose(data=cnn_output,\n",
    "                                                 axes=(0, 2, 1, 3)),\n",
    "                                shape=(0, 0, 0)) \n",
    "    cnn_output = mx.sym.Dropout(cnn_output, p=drop)\n",
    "        \n",
    "    output = mx.sym.FullyConnected(data=cnn_output,\n",
    "                                   num_hidden=input_feature_shape[2])\n",
    "    loss_grad = mx.sym.LinearRegressionOutput(data=output, label=Y)\n",
    "    return (loss_grad,\n",
    "            [v.name for v in iter_train.provide_data],\n",
    "            [v.name for v in iter_train.provide_label])    \n",
    "\n",
    "    \n",
    "def rnn_model(iter_train, input_feature_shape, X, Y,\n",
    "              win, sz_filt, n_filter, drop, seasonal_period):\n",
    "    rnn_cells = mx.rnn.SequentialRNNCell()\n",
    "    rnn_cells.add(mx.rnn.GRUCell(num_hidden=RNN_UNITS))\n",
    "    rnn_cells.add(mx.rnn.DropoutCell(drop))\n",
    "    outputs, _ = rnn_cells.unroll(length=win, inputs=X, merge_outputs=False)\n",
    "    rnn_output = outputs[-1] # only take value from final unrolled cell for use later\n",
    "    \n",
    "    output = mx.sym.FullyConnected(data=rnn_output, num_hidden=input_feature_shape[2])\n",
    "    loss_grad = mx.sym.LinearRegressionOutput(data=output, label=Y)\n",
    "    return (loss_grad,\n",
    "            [v.name for v in iter_train.provide_data],\n",
    "            [v.name for v in iter_train.provide_label])    \n",
    "\n",
    "## simplifications to\n",
    "## https://github.com/apache/incubator-mxnet/blob/master/example/multivariate_time_series/src/lstnet.py\n",
    "def simple_lstnet_model(iter_train,  input_feature_shape, X, Y,\n",
    "                        win, sz_filt, n_filter, drop, seasonal_period):\n",
    "    ## must be 4d or 5d to use padding functionality\n",
    "    conv_input = mx.sym.reshape(data=X, shape=(0, 1, win, -1)) \n",
    "\n",
    "    ## convolutional element\n",
    "    ## we add padding at the end of the time win\n",
    "    cnn_output = mx.sym.pad(data=conv_input,\n",
    "                            mode=\"constant\",\n",
    "                            constant_value=0,\n",
    "                            pad_width=(0, 0,\n",
    "                                       0, 0,\n",
    "                                       0, sz_filt - 1, \n",
    "                                       0, 0))\n",
    "    cnn_output = mx.sym.Convolution(data=cnn_output,\n",
    "                                    kernel=(sz_filt,\n",
    "                                            input_feature_shape[2]),\n",
    "                                    num_filter=n_filter)\n",
    "    cnn_output = mx.sym.Activation(data=cnn_output, act_type='relu')\n",
    "    cnn_output = mx.sym.reshape(mx.sym.transpose(data=cnn_output,\n",
    "                                                 axes=(0, 2, 1, 3)),\n",
    "                                shape=(0, 0, 0))\n",
    "    cnn_output = mx.sym.Dropout(cnn_output, p=drop)\n",
    "\n",
    "    ## recurrent element\n",
    "    stacked_rnn_cells = mx.rnn.SequentialRNNCell()\n",
    "    stacked_rnn_cells.add(mx.rnn.GRUCell(num_hidden=RNN_UNITS))\n",
    "    outputs, _ = stacked_rnn_cells.unroll(length=win,\n",
    "                                          inputs=cnn_output,\n",
    "                                          merge_outputs=False)\n",
    "    rnn_output = outputs[-1] # only take value from final unrolled cell for use later\n",
    "    n_outputs = input_feature_shape[2]\n",
    "    cnn_rnn_model = mx.sym.FullyConnected(data=rnn_output,\n",
    "                                          num_hidden=n_outputs)\n",
    "\n",
    "    ## ar element\n",
    "    ar_outputs = []\n",
    "    for i in list(range(input_feature_shape[2])):\n",
    "        ar_series = mx.sym.slice_axis(data=X,\n",
    "                                      axis=2,\n",
    "                                      begin=i,\n",
    "                                      end=i+1)\n",
    "        fc_ar = mx.sym.FullyConnected(data=ar_series, num_hidden=1)\n",
    "        ar_outputs.append(fc_ar)\n",
    "    ar_model = mx.sym.concat(*ar_outputs, dim=1)\n",
    "\n",
    "    output = cnn_rnn_model + ar_model\n",
    "    loss_grad = mx.sym.LinearRegressionOutput(data=output, label=Y)\n",
    "    return (loss_grad,\n",
    "            [v.name for v in iter_train.provide_data],\n",
    "            [v.name for v in iter_train.provide_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "## TRAINING ##\n",
    "################\n",
    "\n",
    "def train(symbol, iter_train, valid_iter, iter_test,\n",
    "          data_names, label_names,\n",
    "          save_dir, gpu):\n",
    "    ## save training information/results \n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.makedirs(SAVE_DIR)\n",
    "    printFile = open(os.path.join(SAVE_DIR, 'log.txt'), 'w')\n",
    "    def print_to_file(msg):\n",
    "        print(msg)\n",
    "        print(msg, file = printFile, flush = True)\n",
    "    ## print_to_file(args) ## preserve configuation to enable hyperparameter optimization\n",
    "    ## archiving results header\n",
    "    print_to_file('Epoch     Training Cor     Validation Cor')\n",
    "\n",
    "\n",
    "    ## storing prior epoch's values to set an improvement threshold\n",
    "    ## terminates early if progress slow\n",
    "    buf     = RingBuffer(THRESHOLD_EPOCHS)\n",
    "    old_val = None\n",
    "\n",
    "    ## mxnet boilerplate\n",
    "    ## defaults to 1 gpu of which index is 0\n",
    "    ##devs = [mx.gpu(gpu)]\n",
    "    devs   = mx.cpu()\n",
    "    module = mx.mod.Module(symbol,\n",
    "                           data_names=data_names,\n",
    "                           label_names=label_names,\n",
    "                           context=devs)\n",
    "    module.bind(data_shapes=iter_train.provide_data,\n",
    "                label_shapes=iter_train.provide_label)\n",
    "    module.init_params(mx.initializer.Uniform(0.1))\n",
    "    module.init_optimizer(optimizer='adam',\n",
    "                          optimizer_params={'learning_rate': LR})\n",
    "\n",
    "    ## training\n",
    "    for epoch in range( N_EPOCHS):\n",
    "        iter_train.reset()\n",
    "        iter_val.reset()\n",
    "        for batch in iter_train:\n",
    "            module.forward(batch, is_train=True) # compute predictions\n",
    "            module.backward()                    # compute gradients\n",
    "            module.update()                      # update parameters\n",
    "\n",
    "        ## training results\n",
    "        train_pred  = module.predict(iter_train).asnumpy()\n",
    "        train_label = iter_train.label[0][1].asnumpy()\n",
    "        train_perf  = write_eval(train_pred, train_label,\n",
    "                                      save_dir, 'train', epoch)\n",
    "\n",
    "        ## validation results\n",
    "        val_pred  = module.predict(iter_val).asnumpy()\n",
    "        val_label = iter_val.label[0][1].asnumpy()\n",
    "        val_perf = write_eval(val_pred, val_label,\n",
    "                                   save_dir, 'valid', epoch)\n",
    "\n",
    "        print_to_file('%d         %f       %f ' % (epoch, train_perf['COR'], val_perf['COR']))\n",
    "        \n",
    "        if epoch > 0:                                # if we don't yet have measures of improvement, skip\n",
    "            buf.append(val_perf['COR'] - old_val) \n",
    "        if epoch > 2:                                # if we do have measures of improvement, check them\n",
    "            vals = buf.get()\n",
    "            # print(vals)\n",
    "            # print(COR_THRESHOLD)\n",
    "            vals = [v for v in vals if v != 0]\n",
    "            if sum([v < COR_THRESHOLD for v in vals]) == len(vals):\n",
    "                print_to_file('EARLY EXIT')\n",
    "                break\n",
    "        old_val = val_perf['COR']\n",
    "                \n",
    "    ## testing\n",
    "    test_pred  = module.predict(iter_test).asnumpy()\n",
    "    test_label = iter_test.label[0][1].asnumpy()\n",
    "    test_perf = write_eval(test_pred, test_label, save_dir, 'tst', epoch)\n",
    "    print_to_file('\\n TESTING PERFORMANCE')\n",
    "    print_to_file(test_perf)\n",
    "    printFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_eval(pred, label, save_dir, mode, epoch):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    pred_df = pd.DataFrame(pred)\n",
    "    label_df = pd.DataFrame(label)\n",
    "    pred_df.to_csv(os.path.join(save_dir, '%s_pred%d.csv'\n",
    "                                % (mode,epoch)))\n",
    "    label_df.to_csv(os.path.join(save_dir, '%s_label%d.csv'\n",
    "                                % (mode,epoch)))\n",
    "\n",
    "    return { 'COR': COR(label, pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COR(label, pred):\n",
    "    label_demeaned = label - label.mean(0)\n",
    "    label_sumsquares = np.sum(np.square(label_demeaned), 0)\n",
    "\n",
    "    pred_demeaned = pred - pred.mean(0)\n",
    "    pred_sumsquares = np.sum(np.square(pred_demeaned), 0)\n",
    "\n",
    "    cor_coef =  (\n",
    "                    np.diagonal(np.dot(label_demeaned.T, pred_demeaned)) /\n",
    "                    np.sqrt(label_sumsquares * pred_sumsquares))\n",
    "\n",
    "    return np.nanmean(cor_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data iterators\n",
    "iter_train, iter_val, iter_test = prepare_iters(DATA_FILE, WIN, H, MODEL, BATCH_N)    \n",
    "\n",
    "## prepare symbols\n",
    "input_feature_shape = iter_train.provide_data[0][1]    \n",
    "X                   = mx.sym.Variable(iter_train.provide_data[0].name)\n",
    "Y                   = mx.sym.Variable(iter_train.provide_label[0].name)\n",
    "    \n",
    "# set up model\n",
    "model_dict = {\n",
    "    'fc_model'            : fc_model,\n",
    "    'rnn_model'           : rnn_model,\n",
    "    'cnn_model'           : cnn_model,\n",
    "    'simple_lstnet_model' : simple_lstnet_model\n",
    "    }\n",
    "\n",
    "model = model_dict[MODEL]\n",
    "    \n",
    "symbol, data_names, label_names = model(iter_train,\n",
    "                                        input_feature_shape, X, Y,\n",
    "                                        WIN, SZ_FILT,\n",
    "                                        N_FILT, DROP, SEASONAL_PERIOD)\n",
    "\n",
    "## train \n",
    "train(symbol, iter_train, iter_val, iter_test, data_names, label_names, SAVE_DIR, GPU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('volume_forecast_oreilly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e1d7b638fb937fb704dacac9c2a3f5633d00aff928ec7a2a2aeccd6925eb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
